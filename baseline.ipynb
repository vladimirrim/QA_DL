{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "baseline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "000a291ee10441b3b786311d17478a83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4336bc5511274127a12804a07439be8d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_56201b57e9414d0a8bac42eaded6edb5",
              "IPY_MODEL_4b22246d168d4e22997d344ff2ad2af2"
            ]
          }
        },
        "4336bc5511274127a12804a07439be8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "56201b57e9414d0a8bac42eaded6edb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ce8741b35faa457990a0fd483b657768",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 87599,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 87599,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_43c9173c13054ac3a3709a1423917c69"
          }
        },
        "4b22246d168d4e22997d344ff2ad2af2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2674bfe7a66142c49ab4b9621ddf693b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 87599/87599 [02:56&lt;00:00, 495.76it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_619c3bcd18374a38a12327ed10a1e695"
          }
        },
        "ce8741b35faa457990a0fd483b657768": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "43c9173c13054ac3a3709a1423917c69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2674bfe7a66142c49ab4b9621ddf693b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "619c3bcd18374a38a12327ed10a1e695": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vladimirrim/QA_DL/blob/develop/baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TAxwW0vr1H_b",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "!pip install pytorch_pretrained_bert\n",
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j9Rnp0qvsvLP",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\n",
        "from transformers import  BertModel\n",
        "from pytorch_pretrained_bert import BertConfig, BertForPreTraining"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ik0Il6WmsvLe",
        "colab": {}
      },
      "source": [
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Gt-HUB-WsvLg",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class BertForQuestionAnswering(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "        self.bert.eval()\n",
        "        self.qa_outputs = nn.Sequential(nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(64, 2))\n",
        "        self.loss_fct = CrossEntropyLoss()\n",
        "        \n",
        "    \n",
        "\n",
        "    def forward(self, input_ids=None, token_type_ids=None, start_positions=None, end_positions=None, mask=None):\n",
        "        output = self.bert(input_ids, attention_mask=mask)\n",
        "\n",
        "        sequence_output = output[0]\n",
        "\n",
        "        logits = self.qa_outputs(sequence_output)\n",
        "        loss = None\n",
        "\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            loss = (self.loss_fct(logits[:, :, 0].masked_fill((1 - mask).bool(), float('-inf')), start_positions) + \\\n",
        "                   self.loss_fct(logits[:, :, 1].masked_fill((1 - mask).bool(), float('-inf')), end_positions)) / 2\n",
        "\n",
        "        return loss, F.softmax(logits.masked_fill((1 - mask[:, :, None]).bool(), float('-inf')), dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UL1_MdedxrR7",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0tL2AGX3ulcS",
        "colab": {}
      },
      "source": [
        "def preprocess(text, question, answer):\n",
        "    answer = answer.lower()\n",
        "    if answer not in text.lower():\n",
        "        return [], []\n",
        "    \n",
        "    firstInText = text.lower().find(answer)\n",
        "    lastInText = firstInText + len(answer)\n",
        "    text_tokens = tokenizer.tokenize(text[:firstInText].strip())\n",
        "    first = len(text_tokens)\n",
        "    text_tokens += tokenizer.tokenize(text[firstInText:lastInText].strip())\n",
        "    last = len(text_tokens) - 1\n",
        "    text_tokens += tokenizer.tokenize(text[lastInText:].strip())\n",
        "    question_tokens = tokenizer.tokenize(question)\n",
        "    \n",
        "    length = MAX_TEXT_LEN - len(question_tokens) - 3\n",
        "    if len(text_tokens) > length:\n",
        "        part_length = length // 3\n",
        "        stride = 3 * part_length\n",
        "        nrow = np.ceil(len(text_tokens) / part_length) - 2\n",
        "        indexes = part_length * np.arange(nrow)[:, None] + np.arange(stride)\n",
        "        indexes = indexes.astype(np.int32)\n",
        "\n",
        "        max_index = indexes.max()\n",
        "        diff = max_index + 1 - len(text_tokens)\n",
        "        text_tokens += diff * [tokenizer.pad_token]\n",
        "\n",
        "        text_tokens = list(np.array(text_tokens)[indexes])\n",
        "        \n",
        "        tokens = []\n",
        "        labels = []\n",
        "        for i, ts in enumerate(text_tokens):\n",
        "            while ts[-1] == tokenizer.pad_token:\n",
        "                ts = ts[:-1]\n",
        "                \n",
        "            tokens += [ts]\n",
        "                \n",
        "            lfirst = first - i * part_length\n",
        "            llast = last - i * part_length\n",
        "            \n",
        "            mask = lfirst >= 0 and lfirst < len(ts) and llast >= 0 and llast < len(ts)\n",
        "            labels += [(lfirst if mask else 0, llast if mask else 0)]\n",
        "    else:\n",
        "        tokens = [text_tokens]\n",
        "        labels = [(first, last)]\n",
        "        \n",
        "    for i in range(len(tokens)):\n",
        "        # TODO удалять этот костыль!!!\n",
        "        if str(type(tokens[i])) == \"<class 'numpy.ndarray'>\": \n",
        "            tokens[i] = list(tokens[i])\n",
        "        tokens[i] = [tokenizer.cls_token] + \\\n",
        "                    question_tokens + \\\n",
        "                    [tokenizer.sep_token] + \\\n",
        "                    tokens[i] + \\\n",
        "                    [tokenizer.sep_token]\n",
        "        labels[i] = (labels[i][0] + 2 + len(question_tokens), labels[i][1] + 2 + len(question_tokens))\n",
        "\n",
        "    return tokens, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CLptXwgSv6TG",
        "colab": {}
      },
      "source": [
        "def pad_sequence(texts):\n",
        "    max_len = max([len(text) for text in texts])\n",
        "    masks = [[1] * len(text) + [0] * (max_len - len(text)) for text in texts]\n",
        "    texts = [text + [tokenizer.pad_token] * (max_len - len(text)) for text in texts]\n",
        "    texts = [tokenizer.convert_tokens_to_ids(text) for text in texts]\n",
        "    texts = torch.LongTensor(texts)\n",
        "    masks = torch.LongTensor(masks)\n",
        "\n",
        "    return texts, masks\n",
        "\n",
        "def collate_fn(data):\n",
        "    texts, labels = zip(*data)\n",
        "    texts, masks = pad_sequence(texts)\n",
        "    \n",
        "    labels_first, labels_last = zip(*labels)\n",
        "    start_pos = labels_first\n",
        "    end_pos = labels_last\n",
        "    return texts, masks, torch.LongTensor(start_pos), torch.LongTensor(end_pos)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mIfz7nm5yESA",
        "colab": {}
      },
      "source": [
        "MAX_TEXT_LEN = 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0i8jSz-pmjj",
        "colab_type": "code",
        "outputId": "21b21969-1c2b-457d-9d48-a3690ad1aa58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import json\n",
        "\n",
        "drive.mount('./gdrive')\n",
        "train_dataset = './gdrive/My Drive/datasets_for_homeworks/train-v1.1.json'\n",
        "dev_dataset = './gdrive/My Drive/datasets_for_homeworks/dev-v1.1.json'\n",
        "with open(train_dataset, 'r') as train_json, open(dev_dataset, 'r') as dev_json:\n",
        "    train_data = json.load(train_json)\n",
        "    dev_data = json.load(dev_json)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at ./gdrive; to attempt to forcibly remount, call drive.mount(\"./gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33rzwZO6qgBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_text_question_ans_dataset(squad_dataset):\n",
        "    tqa_dataset = []\n",
        "    for d in squad_dataset['data']:\n",
        "        for p in d['paragraphs']:\n",
        "            for qa in p['qas']:\n",
        "                # TODO: deal with several answers\n",
        "                tqa_dataset.append((p['context'], qa['question'], qa['answers'][0]['text']))\n",
        "    return tqa_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23Jnj6zqtNfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tqa_train_dataset = get_text_question_ans_dataset(train_data)\n",
        "tqa_dev_dataset = get_text_question_ans_dataset(dev_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwfHunI3tY43",
        "colab_type": "code",
        "outputId": "3df79801-7f94-4168-d0fd-998c832b3be1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "print(len(tqa_train_dataset))\n",
        "print(len(tqa_dev_dataset))\n",
        "print(f'Max text len in train: {max(map(lambda x: len(x[0]), tqa_train_dataset))}')\n",
        "print(f'Max text len in dev: {max(map(lambda x: len(x[0]), tqa_dev_dataset))}')\n",
        "print(tqa_train_dataset[0])\n",
        "print(tqa_train_dataset[-1])\n",
        "print(tqa_dev_dataset[0])\n",
        "print(tqa_dev_dataset[-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "87599\n",
            "10570\n",
            "Max text len in train: 3706\n",
            "Max text len in dev: 4063\n",
            "('Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'Saint Bernadette Soubirous')\n",
            "(\"Kathmandu Metropolitan City (KMC), in order to promote international relations has established an International Relations Secretariat (IRC). KMC's first international relationship was established in 1975 with the city of Eugene, Oregon, United States. This activity has been further enhanced by establishing formal relationships with 8 other cities: Motsumoto City of Japan, Rochester of the USA, Yangon (formerly Rangoon) of Myanmar, Xi'an of the People's Republic of China, Minsk of Belarus, and Pyongyang of the Democratic Republic of Korea. KMC's constant endeavor is to enhance its interaction with SAARC countries, other International agencies and many other major cities of the world to achieve better urban management and developmental programs for Kathmandu.\", 'What is KMC an initialism of?', 'Kathmandu Metropolitan City')\n",
            "('Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.', 'Which NFL team represented the AFC at Super Bowl 50?', 'Denver Broncos')\n",
            "('The pound-force has a metric counterpart, less commonly used than the newton: the kilogram-force (kgf) (sometimes kilopond), is the force exerted by standard gravity on one kilogram of mass. The kilogram-force leads to an alternate, but rarely used unit of mass: the metric slug (sometimes mug or hyl) is that mass that accelerates at 1 m·s−2 when subjected to a force of 1 kgf. The kilogram-force is not a part of the modern SI system, and is generally deprecated; however it still sees use for some purposes as expressing aircraft weight, jet thrust, bicycle spoke tension, torque wrench settings and engine output torque. Other arcane units of force include the sthène, which is equivalent to 1000 N, and the kip, which is equivalent to 1000 lbf.', 'What is the seldom used force unit equal to one thousand newtons?', 'sthène')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g10DK0DSu9wA",
        "colab_type": "code",
        "outputId": "a367b767-a92b-47f9-a95d-f24268ba8eae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "000a291ee10441b3b786311d17478a83",
            "4336bc5511274127a12804a07439be8d",
            "56201b57e9414d0a8bac42eaded6edb5",
            "4b22246d168d4e22997d344ff2ad2af2",
            "ce8741b35faa457990a0fd483b657768",
            "43c9173c13054ac3a3709a1423917c69",
            "2674bfe7a66142c49ab4b9621ddf693b",
            "619c3bcd18374a38a12327ed10a1e695"
          ]
        }
      },
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "dataset_tokens, dataset_labels = [], []\n",
        "for datapoint in tqdm(tqa_train_dataset):\n",
        "    tokens, labels = preprocess(datapoint[0], datapoint[1], datapoint[2])\n",
        "    dataset_tokens += tokens\n",
        "    dataset_labels += labels"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "000a291ee10441b3b786311d17478a83",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=87599), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FzOR8ch_xV88",
        "outputId": "b924b9a0-50a1-4474-e7ad-919be56a1150",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(dataset_tokens))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "102474\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9pTnwwg9Dey",
        "colab_type": "code",
        "outputId": "6b7d73e9-079d-4ad8-d216-515a0e407799",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print(dataset_tokens[0])\n",
        "print(dataset_tokens[-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', 'To', 'whom', 'did', 'the', 'Virgin', 'Mary', 'allegedly', 'appear', 'in', '1858', 'in', 'Lourdes', 'France', '?', '[SEP]', 'Arch', '##ite', '##ctural', '##ly', ',', 'the', 'school', 'has', 'a', 'Catholic', 'character', '.', 'At', '##op', 'the', 'Main', 'Building', \"'\", 's', 'gold', 'dome', 'is', 'a', 'golden', 'statue', 'of', 'the', 'Virgin', 'Mary', '.', 'Im', '##mediate', '##ly', 'in', 'front', 'of', 'the', 'Main', 'Building', 'and', 'facing', 'it', ',', 'is', 'a', 'copper', 'statue', 'of', 'Christ', 'with', 'arms', 'up', '##rais', '##ed', 'with', 'the', 'legend', '\"', 'Ve', '##nite', 'Ad', 'Me', 'Om', '##nes', '\"', '.', 'Next', 'to', 'the', 'Main', 'Building', 'is', 'the', 'Basilica', 'of', 'the', 'Sacred', 'Heart', '.', 'Im', '##mediate', '##ly', 'behind', 'the', 'basilica', 'is', 'the', 'G', '##rott', '##o', ',', 'a', 'Marian', 'place', 'of', 'prayer', 'and', 'reflect', '##ion', '.', 'It', 'is', 'a', 'replica', 'of', 'the', 'gr', '##otto', 'at', 'Lourdes', ',', 'France', 'where', 'the', 'Virgin', 'Mary', 'rep', '##uted', '##ly', 'appeared', 'to', 'Saint', 'Bernadette', 'So', '##ubi', '##rous', 'in', '1858', '.', 'At', 'the', 'end', 'of', 'the', 'main', 'drive', '(', 'and', 'in', 'a', 'direct', 'line', 'that', 'connects', 'through', '3', 'statues', 'and', 'the', 'Gold', 'Dome', ')', ',', 'is', 'a', 'simple', ',', 'modern', 'stone', 'statue', 'of', 'Mary', '.', '[SEP]']\n",
            "['[CLS]', 'What', 'is', 'KM', '##C', 'an', 'initial', '##ism', 'of', '?', '[SEP]', 'Kat', '##hman', '##du', 'Metropolitan', 'City', '(', 'KM', '##C', ')', ',', 'in', 'order', 'to', 'promote', 'international', 'relations', 'has', 'established', 'an', 'International', 'Relations', 'Secretaria', '##t', '(', 'IR', '##C', ')', '.', 'KM', '##C', \"'\", 's', 'first', 'international', 'relationship', 'was', 'established', 'in', '1975', 'with', 'the', 'city', 'of', 'Eugene', ',', 'Oregon', ',', 'United', 'States', '.', 'This', 'activity', 'has', 'been', 'further', 'enhanced', 'by', 'establishing', 'formal', 'relationships', 'with', '8', 'other', 'cities', ':', 'Mot', '##sum', '##oto', 'City', 'of', 'Japan', ',', 'Rochester', 'of', 'the', 'USA', ',', 'Yang', '##on', '(', 'formerly', 'Rang', '##oon', ')', 'of', 'Myanmar', ',', 'Xi', \"'\", 'an', 'of', 'the', 'People', \"'\", 's', 'Republic', 'of', 'China', ',', 'Minsk', 'of', 'Belarus', ',', 'and', 'P', '##yong', '##yang', 'of', 'the', 'Democratic', 'Republic', 'of', 'Korea', '.', 'KM', '##C', \"'\", 's', 'constant', 'ende', '##avor', 'is', 'to', 'en', '##hance', 'its', 'interaction', 'with', 'SA', '##AR', '##C', 'countries', ',', 'other', 'International', 'agencies', 'and', 'many', 'other', 'major', 'cities', 'of', 'the', 'world', 'to', 'achieve', 'better', 'urban', 'management', 'and', 'development', '##al', 'programs', 'for', 'Kat', '##hman', '##du', '.', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p6vYGneXsvLm",
        "colab": {}
      },
      "source": [
        "train_data_loader = torch.utils.data.DataLoader(list(zip(dataset_tokens, dataset_labels)), batch_size=16, shuffle=True,collate_fn=collate_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-bBIODrkIgG",
        "colab_type": "code",
        "outputId": "f33d0025-b454-4b2d-8011-f131f1cf41d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "source": [
        "!pip3 install wandb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.6/dist-packages (0.8.18)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.21.0)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.5.0)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.6.1)\n",
            "Requirement already satisfied: graphql-core<3.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.2.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.12.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.0.5)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.13.5)\n",
            "Requirement already satisfied: gql>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.1.0)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied: watchdog>=0.8.3 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.9.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (4.0.2)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.8)\n",
            "Requirement already satisfied: promise>=2.1 in /usr/local/lib/python3.6/dist-packages (from graphql-core<3.0.0->wandb) (2.2.1)\n",
            "Requirement already satisfied: rx<3,>=1.6 in /usr/local/lib/python3.6/dist-packages (from graphql-core<3.0.0->wandb) (1.6.1)\n",
            "Requirement already satisfied: gitdb2>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from GitPython>=1.0.0->wandb) (2.0.6)\n",
            "Requirement already satisfied: pathtools>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from watchdog>=0.8.3->wandb) (0.1.2)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from watchdog>=0.8.3->wandb) (3.13)\n",
            "Requirement already satisfied: argh>=0.24.1 in /usr/local/lib/python3.6/dist-packages (from watchdog>=0.8.3->wandb) (0.26.2)\n",
            "Requirement already satisfied: smmap2>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from gitdb2>=2.0.0->GitPython>=1.0.0->wandb) (2.0.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MpPRzgrr0zzA",
        "outputId": "667bad66-ec00-45f1-e56b-89d6a332df48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "!wandb login"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://app.wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: 2377ef66e63c2eda02e1d83797d0cc73170988c7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FnYxG9hY1YS0",
        "outputId": "a51214da-a841-4328-9b2a-37b460084347",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "import wandb\n",
        "wandb.init(project=\"dul\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/ram_saw/dul\" target=\"_blank\">https://app.wandb.ai/ram_saw/dul</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/ram_saw/dul/runs/r85f4nnw\" target=\"_blank\">https://app.wandb.ai/ram_saw/dul/runs/r85f4nnw</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "W&B Run: https://app.wandb.ai/ram_saw/dul/runs/r85f4nnw"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jBiM-Dhq0YJK",
        "colab": {}
      },
      "source": [
        "model = BertForQuestionAnswering()\n",
        "model.load_state_dict(torch.load('./gdrive/My Drive/bert.pt'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KPgoh5AhsvLo",
        "outputId": "2399e5e1-6edf-49e2-bb9b-1d3b2c92cc23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        }
      },
      "source": [
        "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), 0.00005, weight_decay=0.000001)\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.3)\n",
        "epochs = 3\n",
        "device = 'cuda'\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for i, (texts, masks, start_pos, end_pos) in enumerate(train_data_loader):\n",
        "        optimizer.zero_grad()\n",
        "        loss, _ = model(texts.to(device),\n",
        "                        mask=masks.to(device),\n",
        "                        start_positions=torch.tensor(start_pos).to(device),\n",
        "                        end_positions=torch.tensor(end_pos).to(device))\n",
        "        wandb.log({'loss' : float(loss)})\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if i % 100 == 0:\n",
        "            print(f'Model saved on {i} iteration!')\n",
        "            torch.save(model.state_dict(), './gdrive/My Drive/bert.pt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model saved on 0 iteration!\n",
            "Model saved on 100 iteration!\n",
            "Model saved on 200 iteration!\n",
            "Model saved on 300 iteration!\n",
            "Model saved on 400 iteration!\n",
            "Model saved on 500 iteration!\n",
            "Model saved on 600 iteration!\n",
            "Model saved on 700 iteration!\n",
            "Model saved on 800 iteration!\n",
            "Model saved on 900 iteration!\n",
            "Model saved on 1000 iteration!\n",
            "Model saved on 1100 iteration!\n",
            "Model saved on 1200 iteration!\n",
            "Model saved on 1300 iteration!\n",
            "Model saved on 1400 iteration!\n",
            "Model saved on 1500 iteration!\n",
            "Model saved on 1600 iteration!\n",
            "Model saved on 1700 iteration!\n",
            "Model saved on 1800 iteration!\n",
            "Model saved on 1900 iteration!\n",
            "Model saved on 2000 iteration!\n",
            "Model saved on 2100 iteration!\n",
            "Model saved on 2200 iteration!\n",
            "Model saved on 2300 iteration!\n",
            "Model saved on 2400 iteration!\n",
            "Model saved on 2500 iteration!\n",
            "Model saved on 2600 iteration!\n",
            "Model saved on 2700 iteration!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bsnlvBUBe0Iq",
        "colab": {}
      },
      "source": [
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ToDjvM-87waA",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "import re\n",
        "\n",
        "def getBestProb(probs):\n",
        "    n = len(probs)\n",
        "    start, end, bestProb = 0, 0, 0\n",
        "    for i in range(n):\n",
        "        for j in range(i, n):\n",
        "            prob = probs[i, 0] * probs[j, 1]\n",
        "            if bestProb < prob:\n",
        "                bestProb, start, end = prob, i, j\n",
        "              \n",
        "    return start, end\n",
        "\n",
        "\n",
        "def concat(tokens):\n",
        "    tokens = [token.replace('#', '') for token in tokens]\n",
        "    return ' '.join(list(filter(lambda s: s != tokenizer.unk_token, tokens))).strip()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru-bPsq4EUJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "dev_dataset_tokens, dev_dataset_labels = [], []\n",
        "for datapoint in tqdm(tqa_dev_dataset):\n",
        "    tokens, labels = preprocess(datapoint[0], datapoint[1], datapoint[2])\n",
        "    dev_dataset_tokens += tokens\n",
        "    dev_dataset_labels += labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74zNQAAYEc9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_data_loader = torch.utils.data.DataLoader(list(zip(dev_dataset_tokens, dev_dataset_labels)), batch_size=16, shuffle=True,collate_fn=collate_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIHTqBY3Dz9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_model(model):\n",
        "  #TODO load test properly\n",
        "  model.eval()\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for i, (texts, masks, start_pos, end_pos) in enumerate(dev_data_loader):\n",
        "        _, probs = model(texts.to(device),\n",
        "                        mask=masks.to(device),\n",
        "                        start_positions=torch.tensor(start_pos).to(device),\n",
        "                        end_positions=torch.tensor(end_pos).to(device))\n",
        "        start, end = get_best(probs)\n",
        "        correct += torch.sum((start == start_positions) * (end == end_positions))\n",
        "        total += len(start)\n",
        "  print(f'Accuracy on dev data is {correct / total}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8YgyaFj_GpFi",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}