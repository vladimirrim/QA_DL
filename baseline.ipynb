{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "colab_type": "code",
    "id": "TAxwW0vr1H_b",
    "outputId": "6e71ae1f-9089-470e-b935-75bc248627d0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pytorch_pretrained_bert\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j9Rnp0qvsvLP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\n",
    "from transformers import  BertModel\n",
    "from pytorch_pretrained_bert import BertConfig, BertForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ik0Il6WmsvLe"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gt-HUB-WsvLg"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class BertForQuestionAnswering(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "        self.bert.eval()\n",
    "        self.qa_outputs = nn.Sequential(nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(64, 2))\n",
    "        self.loss_fct = CrossEntropyLoss()\n",
    "        \n",
    "    \n",
    "\n",
    "    def forward(self, input_ids=None, token_type_ids=None, start_positions=None, end_positions=None, mask=None):\n",
    "        output = self.bert(input_ids, attention_mask=mask)\n",
    "\n",
    "        sequence_output = output[0]\n",
    "\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        loss = None\n",
    "\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            loss = (self.loss_fct(logits[:, :, 0].masked_fill((1 - mask).bool(), float('-inf')), start_positions) + \\\n",
    "                   self.loss_fct(logits[:, :, 1].masked_fill((1 - mask).bool(), float('-inf')), end_positions)) / 2\n",
    "\n",
    "        return loss, F.softmax(logits.masked_fill((1 - mask[:, :, None]).bool(), float('-inf')), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UL1_MdedxrR7"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0tL2AGX3ulcS"
   },
   "outputs": [],
   "source": [
    "def preprocess(text, question, ans):\n",
    "    answer = answer.lower()\n",
    "    if answer not in text.lower():\n",
    "        return [], []\n",
    "    \n",
    "    firstInText = text.lower().find(answer)\n",
    "    lastInText = first + len(answer)\n",
    "    text_tokens = tokenizer.tokenize(text[:firstInText].strip())\n",
    "    first = len(text_tokens)\n",
    "    text_tokens += tokenizer.tokenize(text[firstInText:lastInText].strip())\n",
    "    last = len(text_tokens) - 1\n",
    "    text_tokens += tokenizer.tokenize(text[lastInText:].strip())\n",
    "    question_tokens = tokenizer.tokenize(question)\n",
    "    \n",
    "    length = MAX_TEXT_LEN - len(question_tokens) - 3\n",
    "    if len(text_tokens) > length:\n",
    "        part_length = length // 3\n",
    "        stride = 3 * part_length\n",
    "        nrow = np.ceil(len(text_tokens) / part_length) - 2\n",
    "        indexes = part_length * np.arange(nrow)[:, None] + np.arange(stride)\n",
    "        print(indexes)\n",
    "        indexes = indexes.astype(np.int32)\n",
    "\n",
    "        max_index = indexes.max()\n",
    "        diff = max_index + 1 - len(text_tokens)\n",
    "        text_tokens += diff * [tokenizer.pad_token]\n",
    "\n",
    "        text_tokens = list(np.array(text_tokens)[indexes])\n",
    "        \n",
    "        tokens = []\n",
    "        labels = []\n",
    "        for i, ts in enumerate(text_tokens):\n",
    "            while ts[-1] == tokenizer.pad_token:\n",
    "                ts = ts[:-1]\n",
    "                \n",
    "            tokens += [ts]\n",
    "                \n",
    "            lfirst = first - i * part_length\n",
    "            llast = last - i * part_length\n",
    "            \n",
    "            mask = lfirst >= 0 and lfirst < len(ts) and llast >= 0 and llast < len(ts)\n",
    "            labels += [(lfirst if mask else 0, llast if mask else 0)]\n",
    "    else:\n",
    "        tokens = [text_tokens]\n",
    "        labels = [(first, last)]\n",
    "        \n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = [tokenizer.cls_token] + \\\n",
    "                    question_tokens + \\\n",
    "                    [tokenizer.sep_token] + \\\n",
    "                    tokens[i] + \\\n",
    "                    [tokenizer.sep_token]\n",
    "        labels[i] = ((labels[i][0][0] + 2 + len(question_tokens), labels[i][0][1]),\n",
    "                     (labels[i][1][0] + 2 + len(question_tokens), labels[i][1][1]))\n",
    "\n",
    "    return tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CLptXwgSv6TG"
   },
   "outputs": [],
   "source": [
    "def pad_sequence(texts):\n",
    "    max_len = max([len(text) for text in texts])\n",
    "    masks = [[1] * len(text) + [0] * (max_len - len(text)) for text in texts]\n",
    "    texts = [text + [tokenizer.pad_token] * (max_len - len(text)) for text in texts]\n",
    "    texts = [tokenizer.convert_tokens_to_ids(text) for text in texts]\n",
    "    texts = torch.LongTensor(texts)\n",
    "    masks = torch.LongTensor(masks)\n",
    "\n",
    "    return texts, masks\n",
    "\n",
    "def collate_fn(data):\n",
    "    texts, labels = zip(*data)\n",
    "    texts, masks = pad_sequence(texts)\n",
    "    \n",
    "    labels_first, labels_last = zip(*labels)\n",
    "    start_pos = zip(*labels_first)\n",
    "    end_pos = zip(*labels_last)\n",
    "    \n",
    "    return texts, masks, torch.LongTensor(start_pos), torch.LongTensor(end_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mIfz7nm5yESA"
   },
   "outputs": [],
   "source": [
    "MAX_TEXT_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fm9USfM0svLk",
    "outputId": "1ad0927a-e167-4b37-8cbe-90c627fa8637"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50365it [01:53, 445.48it/s]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset_tokens, dataset_labels = [], []\n",
    "#TODO load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FzOR8ch_xV88",
    "outputId": "2a25f5ac-f1e5-4dfb-a87a-d1ad8b0153e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50363\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6vYGneXsvLm"
   },
   "outputs": [],
   "source": [
    "train_data_loader = torch.utils.data.DataLoader(list(zip(dataset_tokens, dataset_labels)), batch_size=16, shuffle=True,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MpPRzgrr0zzA"
   },
   "outputs": [],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "FnYxG9hY1YS0",
    "outputId": "d04aa2c1-d3c0-4af5-9d09-c069f34090e6"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"dul\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jBiM-Dhq0YJK"
   },
   "outputs": [],
   "source": [
    "model = BertForQuestionAnswering(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "colab_type": "code",
    "id": "KPgoh5AhsvLo",
    "outputId": "5a915b0a-6a49-4c88-b972-b86e94d02d05"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), 0.00005, weight_decay=0.000001)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.3)\n",
    "epochs = 3\n",
    "device = 'cuda'\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for texts, masks, start_pos, end_pos in train_data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss, _ = model(texts.to(device),\n",
    "                        mask=masks.to(device),\n",
    "                        start_positions=torch.tensor(start_pos).to(device),\n",
    "                        end_positions=torch.tensor(end_pos).to(device))\n",
    "        wandb.log({'loss' : float(loss)})\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bsnlvBUBe0Iq"
   },
   "outputs": [],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ToDjvM-87waA",
    "outputId": "0ec17663-8552-4ef8-8692-a953b711cea2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [00:23, 47.22it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import re\n",
    "\n",
    "def getBestProb(probs):\n",
    "    n = len(probs)\n",
    "    start, end, bestProb = 0, 0, 0\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            prob = probs[i, 0] * probs[j, 1]\n",
    "            if bestProb < prob:\n",
    "                bestProb, start, end = prob, i, j\n",
    "              \n",
    "    return start, end\n",
    "\n",
    "\n",
    "def concat(tokens):\n",
    "    tokens = [token.replace('#', '') for token in tokens]\n",
    "    return ' '.join(list(filter(lambda s: s != tokenizer.unk_token, tokens))).strip()\n",
    "              \n",
    "\n",
    "\n",
    "with open('output', 'w') as f:\n",
    "     f.write('')\n",
    "\n",
    "#TODO load test properly\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  with open('test.txt', newline='') as csvfile:\n",
    "    test = csv.reader(csvfile, delimiter='\\t')\n",
    "    for row in tqdm(test):\n",
    "        question_id, paragraph, question = row[-3], row[-2], row[-1]\n",
    "\n",
    "        question_tokens = tokenizer.tokenize(question)\n",
    "        text_tokens = tokenizer.tokenize(paragraph)\n",
    "        \n",
    "        all_tokens = [tokenizer.cls_token] + \\\n",
    "                     question_tokens + \\\n",
    "                     [tokenizer.sep_token] + \\\n",
    "                     text_tokens + \\\n",
    "                     [tokenizer.sep_token]\n",
    "\n",
    "        length = MAX_TEXT_LEN - len(question_tokens) - 3\n",
    "        if (len(text_tokens) > length):\n",
    "            part_length = length // 3\n",
    "            stride = 3 * part_length\n",
    "            nrow = np.ceil(len(text_tokens) / part_length) - 2\n",
    "            indexes = part_length * np.arange(nrow)[:, None] + np.arange(stride)\n",
    "            indexes = indexes.astype(np.int32)\n",
    "\n",
    "            max_index = indexes.max()\n",
    "            diff = max_index + 1 - len(text_tokens)\n",
    "            text_tokens += diff * [tokenizer.pad_token]\n",
    "            text_tokens = list(np.array(text_tokens)[indexes])\n",
    "\n",
    "            all_probs = []\n",
    "            for i, ts in enumerate(text_tokens):\n",
    "                while ts[-1] == tokenizer.pad_token:\n",
    "                    ts = ts[:-1]\n",
    "\n",
    "                tokens = [tokenizer.cls_token] + \\\n",
    "                     question_tokens + \\\n",
    "                     [tokenizer.sep_token] + \\\n",
    "                     ts + \\\n",
    "                     [tokenizer.sep_token]\n",
    "\n",
    "                texts, masks = pad_sequence([tokens])\n",
    "                texts = texts.to(device)\n",
    "                masks = masks.to(device)\n",
    "\n",
    "                _, probs = model(texts, mask=masks)\n",
    "                probs = probs.squeeze(0)\n",
    "                probs = lps[2 + len(question_tokens):]\n",
    "                probs[:, 0] = F.softmax(probs[:, 0], 0)\n",
    "                probs[:, 1] = F.softmax(probs[:, 1], 0)\n",
    "                probs = probs.cpu().numpy()\n",
    "                all_probs += list(probs)\n",
    "            \n",
    "            start, end = get_best(np.array(all_probs))\n",
    "                \n",
    "            start += 2 + len(question_tokens)\n",
    "            end += 2 + len(question_tokens)\n",
    "                    \n",
    "        else:\n",
    "            texts, masks = pad_sequence([all_tokens])\n",
    "            texts = texts.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            _, probs = model(texts, mask=masks)\n",
    "            probs = probs.squeeze(0)\n",
    "            probs[:, 0] = F.softmax(probs[:, 0], 0)\n",
    "            probs[:, 1] = F.softmax(probs[:, 1], 0)\n",
    "            probs = probs.cpu().numpy()\n",
    "            \n",
    "            start, end = get_best(probs)\n",
    "            \n",
    "        s = concat(all_tokens[start: end + 1])\n",
    "        with open('output', 'a') as f:\n",
    "            f.write(question_id + '\\t' + s + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8YgyaFj_GpFi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "Hw 4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
